{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Convolutional layer implementation using NumPy"
      ],
      "metadata": {
        "id": "Ro6Pq5ARhZNj"
      },
      "id": "Ro6Pq5ARhZNj"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "Ni-JPw8us6oe"
      },
      "id": "Ni-JPw8us6oe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convolutional layer implementation:\n",
        "It takes the number and size of kernels as inputs"
      ],
      "metadata": {
        "id": "cJLsa7w9hiqs"
      },
      "id": "cJLsa7w9hiqs"
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvolutionLayer:\n",
        "    def __init__(self, kernel_num, kernel_size):\n",
        "        # kernel size n -> (n x n)\n",
        "        self.kernel_num = kernel_num\n",
        "        self.kernel_size = kernel_size\n",
        "        # Random generation and normalization of kernel weights\n",
        "        self.kernels = np.random.randn(kernel_num, kernel_size, kernel_size) / (kernel_size**2)\n",
        "\n",
        "    def patches_generator(self, image):\n",
        "        # patches for the kernel saved as tuple with the coordinates\n",
        "        # image height and width\n",
        "        image_h, image_w = image.shape\n",
        "        self.image = image\n",
        "        # number of patches (fxf filter) = h-f+1 for height and w-f+1 for width\n",
        "        for h in range(image_h-self.kernel_size+1):\n",
        "            for w in range(image_w-self.kernel_size+1):\n",
        "                patch = image[h:(h+self.kernel_size), w:(w+self.kernel_size)]\n",
        "                yield patch, h, w\n",
        "\n",
        "    def forward_prop(self, image):\n",
        "        image_h, image_w = image.shape\n",
        "        # convolution output initialization\n",
        "        convolution_output = np.zeros((image_h-self.kernel_size+1, image_w-self.kernel_size+1, self.kernel_num))\n",
        "        # unpack the generator\n",
        "        for patch, h, w in self.patches_generator(image):\n",
        "            # convolution for the patches\n",
        "            convolution_output[h,w] = np.sum(patch*self.kernels, axis=(1,2))\n",
        "        return convolution_output\n",
        "\n",
        "    def back_prop(self, dE_dY, alpha):\n",
        "        # initialize gradient of the loss function with respect to the kernel weights\n",
        "        dE_dk = np.zeros(self.kernels.shape)\n",
        "        for patch, h, w in self.patches_generator(self.image):\n",
        "            for f in range(self.kernel_num):\n",
        "                dE_dk[f] += patch * dE_dY[h, w, f]\n",
        "        # update the parameters\n",
        "        self.kernels -= alpha*dE_dk\n",
        "        return dE_dk"
      ],
      "metadata": {
        "id": "3ysiz1Z2tS_G"
      },
      "id": "3ysiz1Z2tS_G",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AvgPooling layer takes the convolutional layer's output as an input and the size of pooling kernel for the constructor"
      ],
      "metadata": {
        "id": "UX15x-gQhwL2"
      },
      "id": "UX15x-gQhwL2"
    },
    {
      "cell_type": "code",
      "source": [
        "class AvgPoolingLayer:\n",
        "    def __init__(self, kernel_size):\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "    def patches_generator(self, image):\n",
        "        # Generate patches for pooling\n",
        "        output_h = image.shape[0] // self.kernel_size\n",
        "        output_w = image.shape[1] // self.kernel_size\n",
        "        self.image = image\n",
        "\n",
        "        for h in range(output_h):\n",
        "            for w in range(output_w):\n",
        "                patch = image[(h*self.kernel_size):(h*self.kernel_size+self.kernel_size), (w*self.kernel_size):(w*self.kernel_size+self.kernel_size)]\n",
        "                yield patch, h, w\n",
        "\n",
        "    def forward_prop(self, image):\n",
        "        image_h, image_w, num_kernels = image.shape\n",
        "        avg_pooling_output = np.zeros((image_h//self.kernel_size, image_w//self.kernel_size, num_kernels))\n",
        "        for patch, h, w in self.patches_generator(image):\n",
        "            avg_pooling_output[h,w] = np.mean(patch, axis=(0,1))\n",
        "        return avg_pooling_output\n",
        "\n",
        "    def back_prop(self, dE_dY):\n",
        "        dE_dk = np.zeros(self.image.shape)\n",
        "        output_h, output_w, num_kernels = dE_dY.shape\n",
        "        for h in range(output_h):\n",
        "            for w in range(output_w):\n",
        "                patch_h = h * self.kernel_size\n",
        "                patch_w = w * self.kernel_size\n",
        "                patch = self.image[patch_h : patch_h + self.kernel_size, patch_w : patch_w + self.kernel_size, :]\n",
        "                dE_dY_patch = dE_dY[h, w, :] / (self.kernel_size ** 2)\n",
        "                dE_dk[patch_h : patch_h + self.kernel_size, patch_w : patch_w + self.kernel_size, :] = dE_dY_patch\n",
        "\n",
        "        return dE_dk"
      ],
      "metadata": {
        "id": "lWkV3vVZrYu_"
      },
      "id": "lWkV3vVZrYu_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Softmax activation is used for the final predictions:"
      ],
      "metadata": {
        "id": "MHRwt2Bwh-Pc"
      },
      "id": "MHRwt2Bwh-Pc"
    },
    {
      "cell_type": "code",
      "source": [
        "class SoftmaxLayer:\n",
        "    def __init__(self, input_units, output_units):\n",
        "        # random initialization of w and b\n",
        "        self.weight = np.random.randn(input_units, output_units)/input_units\n",
        "        self.bias = np.zeros(output_units)\n",
        "\n",
        "    def forward_prop(self, image):\n",
        "        self.original_shape = image.shape # stored for backprop\n",
        "        image_flattened = image.flatten()\n",
        "        self.flattened_input = image_flattened # stored for backprop\n",
        "        # linear output\n",
        "        first_output = np.dot(image_flattened, self.weight) + self.bias\n",
        "        self.output = first_output\n",
        "        # softmax activation\n",
        "        softmax_output = np.exp(first_output) / np.sum(np.exp(first_output), axis=0)\n",
        "        return softmax_output\n",
        "\n",
        "    def back_prop(self, dE_dY, alpha):\n",
        "        for i, gradient in enumerate(dE_dY):\n",
        "            if gradient == 0:\n",
        "                continue\n",
        "            transformation_eq = np.exp(self.output)\n",
        "            S_total = np.sum(transformation_eq)\n",
        "\n",
        "            # gradients with respect to output (Z)\n",
        "            dY_dZ = -transformation_eq[i]*transformation_eq / (S_total**2)\n",
        "            dY_dZ[i] = transformation_eq[i]*(S_total - transformation_eq[i]) / (S_total**2)\n",
        "\n",
        "            # gradients of linear output with respect to weight, bias, input\n",
        "            dZ_dw = self.flattened_input\n",
        "            dZ_db = 1\n",
        "            dZ_dX = self.weight\n",
        "\n",
        "            # gradient of loss with respect ot output\n",
        "            dE_dZ = gradient * dY_dZ\n",
        "\n",
        "            # gradient of loss with respect to weight, bias, input\n",
        "            dE_dw = dZ_dw[np.newaxis].T @ dE_dZ[np.newaxis]\n",
        "            dE_db = dE_dZ * dZ_db\n",
        "            dE_dX = dZ_dX @ dE_dZ\n",
        "\n",
        "            # update parameters\n",
        "            self.weight -= alpha*dE_dw\n",
        "            self.bias -= alpha*dE_db\n",
        "\n",
        "            return dE_dX.reshape(self.original_shape)"
      ],
      "metadata": {
        "id": "uo1La7XUr3oF"
      },
      "id": "uo1La7XUr3oF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functions for performing the training using the entire network"
      ],
      "metadata": {
        "id": "GjaP2m5ti7Gi"
      },
      "id": "GjaP2m5ti7Gi"
    },
    {
      "cell_type": "code",
      "source": [
        "def CNN_forward(image, label, layers):\n",
        "    output = image/255.\n",
        "    for layer in layers:\n",
        "        output = layer.forward_prop(output)\n",
        "    # loss (cross-entropy) and accuracy\n",
        "    loss = -np.log(output[label])\n",
        "    accuracy = 1 if np.argmax(output) == label else 0\n",
        "    return output, loss, accuracy\n",
        "\n",
        "def CNN_backprop(gradient, layers, alpha=0.05):\n",
        "    grad_back = gradient\n",
        "    for layer in layers[::-1]:\n",
        "        if type(layer) in [ConvolutionLayer, SoftmaxLayer]:\n",
        "            grad_back = layer.back_prop(grad_back, alpha)\n",
        "        elif type(layer) == AvgPoolingLayer:\n",
        "            grad_back = layer.back_prop(grad_back)\n",
        "    return grad_back\n",
        "\n",
        "\n",
        "def CNN_training(image, label, layers, alpha=0.05):\n",
        "    # forward step\n",
        "    output, loss, accuracy = CNN_forward(image, label, layers)\n",
        "\n",
        "    # initial gradient\n",
        "    gradient = np.zeros(10)\n",
        "    gradient[label] = -1/output[label]\n",
        "\n",
        "    # backprop step\n",
        "    gradient_back = CNN_backprop(gradient, layers, alpha)\n",
        "\n",
        "    return loss, accuracy"
      ],
      "metadata": {
        "id": "C7BGJdBQsSK4"
      },
      "id": "C7BGJdBQsSK4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "BdUMdaGTsdlx"
      },
      "id": "BdUMdaGTsdlx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This snippet performs the training only for 4 epochs and shows the results on the training set."
      ],
      "metadata": {
        "id": "OTQEUU6gjCgx"
      },
      "id": "OTQEUU6gjCgx"
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "  # MNIST data\n",
        "  (X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "  X_train = X_train[:5000]\n",
        "  y_train = y_train[:5000]\n",
        "\n",
        "  layers = [\n",
        "    ConvolutionLayer(16,3), # layer with 16 3x3 filters, output (26,26,16)\n",
        "    AvgPoolingLayer(2), # pooling layer 2x2, output (13,13,16)\n",
        "    SoftmaxLayer(13*13*16, 10) # softmax layer with 13*13*16 input and 10 outputs\n",
        "    ]\n",
        "\n",
        "  for epoch in range(4):\n",
        "    print('Epoch {} ->'.format(epoch+1))\n",
        "    # shuffle training data\n",
        "    permutation = np.random.permutation(len(X_train))\n",
        "    X_train = X_train[permutation]\n",
        "    y_train = y_train[permutation]\n",
        "    # training the CNN\n",
        "    loss = 0\n",
        "    accuracy = 0\n",
        "    for i, (image, label) in enumerate(zip(X_train, y_train)):\n",
        "      if i % 100 == 0: # print for every 100 steps\n",
        "        print(\"Step {}. For the last 100 steps: average loss {}, accuracy {}\".format(i+1, loss/100, accuracy))\n",
        "        loss = 0\n",
        "        accuracy = 0\n",
        "      loss_1, accuracy_1 = CNN_training(image, label, layers)\n",
        "      loss += loss_1\n",
        "      accuracy += accuracy_1\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        },
        "id": "2fPQLiRns9dS",
        "outputId": "efb51300-4625-4e31-9a05-37b1d6f6bb5c"
      },
      "id": "2fPQLiRns9dS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 ->\n",
            "Step 1. For the last 100 steps: average loss 0.0, accuracy 0\n",
            "Step 101. For the last 100 steps: average loss 1.8847034394040136, accuracy 38\n",
            "Step 201. For the last 100 steps: average loss 1.0730965966293853, accuracy 71\n",
            "Step 301. For the last 100 steps: average loss 0.7937386502381307, accuracy 79\n",
            "Step 401. For the last 100 steps: average loss 0.9550129525211772, accuracy 70\n",
            "Step 501. For the last 100 steps: average loss 0.7335506267417213, accuracy 77\n",
            "Step 601. For the last 100 steps: average loss 0.6321832950583985, accuracy 78\n",
            "Step 701. For the last 100 steps: average loss 0.5957851476769828, accuracy 80\n",
            "Step 801. For the last 100 steps: average loss 0.6497543607863918, accuracy 81\n",
            "Step 901. For the last 100 steps: average loss 0.8993925826000817, accuracy 71\n",
            "Step 1001. For the last 100 steps: average loss 0.438797420956733, accuracy 90\n",
            "Step 1101. For the last 100 steps: average loss 0.7580861045273292, accuracy 79\n",
            "Step 1201. For the last 100 steps: average loss 0.6994850138526294, accuracy 82\n",
            "Step 1301. For the last 100 steps: average loss 0.8030892051051142, accuracy 81\n",
            "Step 1401. For the last 100 steps: average loss 0.6481356640230694, accuracy 85\n",
            "Step 1501. For the last 100 steps: average loss 0.3973037604981448, accuracy 90\n",
            "Step 1601. For the last 100 steps: average loss 0.5715124513674931, accuracy 84\n",
            "Step 1701. For the last 100 steps: average loss 0.5403371153782122, accuracy 82\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-9e2a6b0df0fe>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m   \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-22-9e2a6b0df0fe>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m       \u001b[0mloss_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNN_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m       \u001b[0maccuracy\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0maccuracy_1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-8fcdb2fe26cb>\u001b[0m in \u001b[0;36mCNN_training\u001b[0;34m(image, label, layers, alpha)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# backprop step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mgradient_back\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNN_backprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-8fcdb2fe26cb>\u001b[0m in \u001b[0;36mCNN_backprop\u001b[0;34m(gradient, layers, alpha)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mConvolutionLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSoftmaxLayer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mgrad_back\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mback_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_back\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mAvgPoolingLayer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mgrad_back\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mback_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_back\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-6d7438062f5f>\u001b[0m in \u001b[0;36mback_prop\u001b[0;34m(self, dE_dY, alpha)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatches_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0mdE_dk\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpatch\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdE_dY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;31m# update the parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernels\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdE_dk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Naw_LLntBUm"
      },
      "id": "9Naw_LLntBUm",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}